{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN7R4Uh8GnsnYArAtgrbtB/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CsOK4DTcEs2h","executionInfo":{"status":"ok","timestamp":1760971512120,"user_tz":-420,"elapsed":77274,"user":{"displayName":"Maulana Rengga Ramadan","userId":"05849588803174512002"}},"outputId":"d4895e82-b5f2-4e37-f171-dc9580f3a8a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using Colab cache for faster access to the 'spotify-songs-with-attributes-and-lyrics' dataset.\n","Path to dataset files: /kaggle/input/spotify-songs-with-attributes-and-lyrics\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"bwandowando/spotify-songs-with-attributes-and-lyrics\")\n","\n","print(\"Path to dataset files:\", path)\n","\n","!pip install -q annoy faiss-cpu hnswlib"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","import time\n","import faiss\n","from annoy import AnnoyIndex\n","import hnswlib\n","from sklearn.neighbors import NearestNeighbors\n","from joblib import Parallel, delayed\n","from sklearn.preprocessing import StandardScaler\n","\n","# Use all available CPU cores where possible\n","n_cores = os.cpu_count() or 1\n","os.environ.setdefault('OMP_NUM_THREADS', str(n_cores))\n","os.environ.setdefault('OPENBLAS_NUM_THREADS', str(n_cores))\n","os.environ.setdefault('MKL_NUM_THREADS', str(n_cores))\n","# Tell faiss to use multiple threads (if built with OpenMP)\n","try:\n","    faiss.omp_set_num_threads(n_cores)\n","except Exception:\n","    pass\n","\n","# -------------------------------\n","# Load dataset (drop NaNs in chosen features)\n","# -------------------------------\n","df = pd.read_csv(f'{path}/songs_with_attributes_and_lyrics.csv')  # ganti path sesuai lokasi file\n","features = ['danceability', 'energy', 'loudness', 'speechiness',\n","            'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n","df = df[features].dropna().reset_index(drop=True)\n","X = df.values\n","\n","# Standardize and cast to float32 (required by faiss/hnswlib)\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X).astype(np.float32)\n","\n","n = X_scaled.shape[0]\n","k = 10  # jumlah nearest neighbors\n","# To keep this runnable on limited RAM, sample up to 1000 query points\n","n_queries = min(1000, n)\n","rng = np.random.default_rng(42)\n","query_idx = rng.choice(n, size=n_queries, replace=False)\n","# Xq = X_scaled[query_idx]\n","Xq = X_scaled\n","\n","# -------------------------------\n","# Exact Nearest Neighbor (brute-force) - only for the sampled queries\n","# -------------------------------\n","t0 = time.time()\n","nn = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean', n_jobs=-1)\n","nn.fit(X_scaled)\n","dist_exact, idx_exact = nn.kneighbors(Xq)\n","time_exact = time.time() - t0\n","print(f\"Exact NN (queries={n_queries}) done in {time_exact:.3f} s\")\n","\n","# -------------------------------\n","# Annoy (build + query on sampled points)\n","# -------------------------------\n","t0 = time.time()\n","fdim = X_scaled.shape[1]\n","index_annoy = AnnoyIndex(fdim, 'euclidean')\n","for i, v in enumerate(X_scaled):\n","    index_annoy.add_item(i, v.tolist())\n","n_trees = 50\n","index_annoy.build(n_trees)\n","t_build_annoy = time.time() - t0\n","\n","tq = time.time()\n","# Annoy: parallelize queries using joblib (threading) to utilize multiple cores\n","def _query_annoy(v):\n","    return index_annoy.get_nns_by_vector(v.tolist(), k)\n","idx_annoy = Parallel(n_jobs=n_cores, prefer='threads')(delayed(_query_annoy)(v) for v in Xq)\n","time_query_annoy = time.time() - tq\n","print(f\"Annoy build: {t_build_annoy:.3f} s, query all: {time_query_annoy:.3f} s\")\n","\n","# -------------------------------\n","# HNSW (hnswlib)\n","# -------------------------------\n","t0 = time.time()\n","p = hnswlib.Index(space='l2', dim=fdim)\n","p.init_index(max_elements=n, ef_construction=200, M=16)\n","p.add_items(X_scaled)\n","p.set_ef(200)\n","t_build_hnsw = time.time() - t0\n","\n","tq = time.time()\n","# hnswlib supports num_threads in knn_query\n","idx_hnsw, dist_hnsw = p.knn_query(Xq, k=k, num_threads=n_cores)\n","time_query_hnsw = time.time() - tq\n","print(f\"HNSW build: {t_build_hnsw:.3f} s, query all: {time_query_hnsw:.3f} s\")\n","\n","# -------------------------------\n","# FAISS IVF (train on full set, query sampled points)\n","# -------------------------------\n","t0 = time.time()\n","quantizer = faiss.IndexFlatL2(fdim)\n","nlist = 100\n","index_faiss = faiss.IndexIVFFlat(quantizer, fdim, nlist, faiss.METRIC_L2)\n","# FAISS requires float32 and contiguous arrays\n","index_faiss.train(np.ascontiguousarray(X_scaled))\n","index_faiss.add(np.ascontiguousarray(X_scaled))\n","index_faiss.nprobe = 10\n","t_build_faiss = time.time() - t0\n","\n","tq = time.time()\n","# FAISS can use multiple threads via set_num_threads if available\n","try:\n","    faiss.omp_set_num_threads(n_cores)\n","except Exception:\n","    pass\n","D_faiss, idx_faiss = index_faiss.search(np.ascontiguousarray(Xq), k)\n","time_query_faiss = time.time() - tq\n","print(f\"FAISS build: {t_build_faiss:.3f} s, query all: {time_query_faiss:.3f} s\")\n","\n","# -------------------------------\n","# Evaluate recall@k for each ANN vs exact\n","# -------------------------------\n","def recall_at_k(true_idx, pred_idx, k):\n","    # true_idx: (n_queries, k), pred_idx: iterable of length n_queries with lists/arrays\n","    total = 0.0\n","    n = len(true_idx)\n","    for t, p in zip(true_idx, pred_idx):\n","        pset = set(p.tolist() if hasattr(p, 'tolist') else p)\n","        total += len(pset.intersection(set(t[:k]))) / float(k)\n","    return total / n\n","\n","rec_annoy = recall_at_k(idx_exact, idx_annoy, k)\n","rec_hnsw = recall_at_k(idx_exact, idx_hnsw, k)\n","rec_faiss = recall_at_k(idx_exact, idx_faiss, k)\n","\n","print('\\nSummary (build time | query time for sampled points | recall@k)')\n","print(f\"Exact:  - | {time_exact:.3f} s (queries only) | recall=1.00\")\n","print(f\"Annoy:  {t_build_annoy:.3f} s | {time_query_annoy:.3f} s | recall@{k}={rec_annoy:.4f}\")\n","print(f\"HNSW:   {t_build_hnsw:.3f} s | {time_query_hnsw:.3f} s | recall@{k}={rec_hnsw:.4f}\")\n","print(f\"FAISS:  {t_build_faiss:.3f} s | {time_query_faiss:.3f} s | recall@{k}={rec_faiss:.4f}\")\n","\n","# show top-5 neighbors for the first sampled query (original dataset index)\n","qid = query_idx[0]\n","print(\"\\nTop-5 neighbors for first sampled query (dataset index = {})\".format(int(qid)))\n","print(f\"Exact NN: {idx_exact[0][:5]}\")\n","print(f\"Annoy:    {idx_annoy[0][:5]}\")\n","print(f\"HNSW:     {idx_hnsw[0][:5]}\")\n","print(f\"FAISS:    {idx_faiss[0][:5]}\")"],"metadata":{"id":"5NMowU4CFRew"},"execution_count":null,"outputs":[]}]}